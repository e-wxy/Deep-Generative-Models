{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDXL\n",
    "\n",
    "SDXL optimised for inference on low-memory GPUs.\n",
    "\n",
    "References:\n",
    "1. Podell, Dustin, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. ‘SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis’. In The Twelfth International Conference on Learning Representations, 2023. https://openreview.net/forum?id=di52zR8xgf.\n",
    "2. [Generative Models by Stability AI](https://github.com/Stability-AI/generative-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from einops import rearrange, repeat\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import torch\n",
    "import torchvision.transforms as TT\n",
    "from einops import rearrange, repeat\n",
    "from omegaconf import ListConfig, OmegaConf\n",
    "from PIL import Image\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "from scripts.demo.discretization import (\n",
    "    Img2ImgDiscretizationWrapper,\n",
    "    Txt2NoisyDiscretizationWrapper,\n",
    ")\n",
    "from scripts.util.detection.nsfw_and_watermark_dectection import DeepFloydDataFiltering\n",
    "from sgm.inference.helpers import embed_watermark\n",
    "from sgm.modules.diffusionmodules.guiders import (\n",
    "    LinearPredictionGuider,\n",
    "    TrianglePredictionGuider,\n",
    "    VanillaCFG,\n",
    ")\n",
    "from sgm.modules.diffusionmodules.sampling import (\n",
    "    DPMPP2MSampler,\n",
    "    DPMPP2SAncestralSampler,\n",
    "    EulerAncestralSampler,\n",
    "    EulerEDMSampler,\n",
    "    HeunEDMSampler,\n",
    "    LinearMultistepSampler,\n",
    ")\n",
    "from sgm.util import append_dims, default, instantiate_from_config\n",
    "from torch import autocast\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def load_image(image_path, W, H):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    print(f\"Loaded input image of size {image.size}\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((H, W)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x * 2.0 - 1.0),\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(\n",
    "    img_path,\n",
    "    size: Union[None, int, Tuple[int, int]] = None,\n",
    "    center_crop: bool = False,\n",
    "):\n",
    "    image = Image.open(img_path)\n",
    "    # display.display(image)\n",
    "\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h})\")\n",
    "\n",
    "    # Ensure target size is divisible by 64\n",
    "    if size is not None:\n",
    "        if isinstance(size, int):\n",
    "            size = (size, size)\n",
    "        size = (size[0] - size[0] % 64, size[1] - size[1] % 64)\n",
    "        print(f\"Adjusted size to be divisible by 64: {size}\")\n",
    "\n",
    "    transform = []\n",
    "    if size is not None:\n",
    "        transform.append(transforms.Resize(size))\n",
    "    if center_crop:\n",
    "        transform.append(transforms.CenterCrop(size))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform.append(transforms.Lambda(lambda x: 2.0 * x - 1.0))\n",
    "\n",
    "    transform = transforms.Compose(transform)\n",
    "    img = transform(image)[None, ...].to(device)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_embedder_keys_from_conditioner(conditioner):\n",
    "    return list(set([x.input_key for x in conditioner.embedders]))\n",
    "\n",
    "def init_embedder_options(keys, init_dict, prompt=None, negative_prompt=None):\n",
    "    value_dict = {}\n",
    "    for key in keys:\n",
    "        if key == \"txt\":\n",
    "            if prompt is None:\n",
    "                prompt = \"A professional photograph of an astronaut riding a pig\"\n",
    "            if negative_prompt is None:\n",
    "                negative_prompt = \"\"\n",
    "\n",
    "            prompt = st.text_input(\"Prompt\", prompt)\n",
    "            negative_prompt = st.text_input(\"Negative prompt\", negative_prompt)\n",
    "\n",
    "            value_dict[\"prompt\"] = prompt\n",
    "            value_dict[\"negative_prompt\"] = negative_prompt\n",
    "\n",
    "        if key == \"original_size_as_tuple\":\n",
    "            orig_width = st.number_input(\n",
    "                \"orig_width\",\n",
    "                value=init_dict[\"orig_width\"],\n",
    "                min_value=16,\n",
    "            )\n",
    "            orig_height = st.number_input(\n",
    "                \"orig_height\",\n",
    "                value=init_dict[\"orig_height\"],\n",
    "                min_value=16,\n",
    "            )\n",
    "\n",
    "            value_dict[\"orig_width\"] = orig_width\n",
    "            value_dict[\"orig_height\"] = orig_height\n",
    "\n",
    "        if key == \"crop_coords_top_left\":\n",
    "            crop_coord_top = st.number_input(\"crop_coords_top\", value=0, min_value=0)\n",
    "            crop_coord_left = st.number_input(\"crop_coords_left\", value=0, min_value=0)\n",
    "\n",
    "            value_dict[\"crop_coords_top\"] = crop_coord_top\n",
    "            value_dict[\"crop_coords_left\"] = crop_coord_left\n",
    "\n",
    "        if key == \"aesthetic_score\":\n",
    "            value_dict[\"aesthetic_score\"] = 6.0\n",
    "            value_dict[\"negative_aesthetic_score\"] = 2.5\n",
    "\n",
    "        if key == \"target_size_as_tuple\":\n",
    "            value_dict[\"target_width\"] = init_dict[\"target_width\"]\n",
    "            value_dict[\"target_height\"] = init_dict[\"target_height\"]\n",
    "\n",
    "        if key in [\"fps_id\", \"fps\"]:\n",
    "            fps = st.number_input(\"fps\", value=6, min_value=1)\n",
    "\n",
    "            value_dict[\"fps\"] = fps\n",
    "            value_dict[\"fps_id\"] = fps - 1\n",
    "\n",
    "        if key == \"motion_bucket_id\":\n",
    "            mb_id = st.number_input(\"motion bucket id\", 0, 511, value=127)\n",
    "            value_dict[\"motion_bucket_id\"] = mb_id\n",
    "\n",
    "        if key == \"pool_image\":\n",
    "            st.text(\"Image for pool conditioning\")\n",
    "            image = load_img(\n",
    "                key=\"pool_image_input\",\n",
    "                size=224,\n",
    "                center_crop=True,\n",
    "            )\n",
    "            if image is None:\n",
    "                st.info(\"Need an image here\")\n",
    "                image = torch.zeros(1, 3, 224, 224)\n",
    "            value_dict[\"pool_image\"] = image\n",
    "\n",
    "    return value_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "\n",
    "VERSION2SPECS = {\n",
    "    \"SDXL-base-1.0\": {\n",
    "        \"H\": 1024,\n",
    "        \"W\": 1024,\n",
    "        \"C\": 4,\n",
    "        \"f\": 8,\n",
    "        \"is_legacy\": False,\n",
    "        \"config\": \"configs/inference/sd_xl_base.yaml\",\n",
    "        \"ckpt\": \"checkpoints/sd_xl_base_1.0.safetensors\",\n",
    "    },\n",
    "    \"SDXL-base-0.9\": {\n",
    "        \"H\": 1024,\n",
    "        \"W\": 1024,\n",
    "        \"C\": 4,\n",
    "        \"f\": 8,\n",
    "        \"is_legacy\": False,\n",
    "        \"config\": \"configs/inference/sd_xl_base.yaml\",\n",
    "        \"ckpt\": \"checkpoints/sd_xl_base_0.9.safetensors\",\n",
    "    },\n",
    "    \"SD-2.1\": {\n",
    "        \"H\": 512,\n",
    "        \"W\": 512,\n",
    "        \"C\": 4,\n",
    "        \"f\": 8,\n",
    "        \"is_legacy\": True,\n",
    "        \"config\": \"configs/inference/sd_2_1.yaml\",\n",
    "        \"ckpt\": \"checkpoints/v2-1_512-ema-pruned.safetensors\",\n",
    "    },\n",
    "    \"SD-2.1-768\": {\n",
    "        \"H\": 768,\n",
    "        \"W\": 768,\n",
    "        \"C\": 4,\n",
    "        \"f\": 8,\n",
    "        \"is_legacy\": True,\n",
    "        \"config\": \"configs/inference/sd_2_1_768.yaml\",\n",
    "        \"ckpt\": \"checkpoints/v2-1_768-ema-pruned.safetensors\",\n",
    "    },\n",
    "    \"SDXL-refiner-0.9\": {\n",
    "        \"H\": 1024,\n",
    "        \"W\": 1024,\n",
    "        \"C\": 4,\n",
    "        \"f\": 8,\n",
    "        \"is_legacy\": True,\n",
    "        \"config\": \"configs/inference/sd_xl_refiner.yaml\",\n",
    "        \"ckpt\": \"checkpoints/sd_xl_refiner_0.9.safetensors\",\n",
    "    },\n",
    "    \"SDXL-refiner-1.0\": {\n",
    "        \"H\": 1024,\n",
    "        \"W\": 1024,\n",
    "        \"C\": 4,\n",
    "        \"f\": 8,\n",
    "        \"is_legacy\": True,\n",
    "        \"config\": \"configs/inference/sd_xl_refiner.yaml\",\n",
    "        \"ckpt\": \"checkpoints/sd_xl_refiner_1.0.safetensors\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowvram_mode = True\n",
    "\n",
    "def load_model(model):\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "def initial_model_load(model):\n",
    "    if lowvram_mode:\n",
    "        model.model.half()\n",
    "    else:\n",
    "        model.cuda()\n",
    "    return model\n",
    "\n",
    "\n",
    "def unload_model(model):\n",
    "    if lowvram_mode:\n",
    "        model.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt=None, verbose=True):\n",
    "    model = instantiate_from_config(config.model)\n",
    "\n",
    "    if ckpt is not None:\n",
    "        print(f\"Loading model from {ckpt}\")\n",
    "        if ckpt.endswith(\"ckpt\"):\n",
    "            pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "            if \"global_step\" in pl_sd:\n",
    "                global_step = pl_sd[\"global_step\"]\n",
    "                st.info(f\"loaded ckpt from global step {global_step}\")\n",
    "                print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "            sd = pl_sd[\"state_dict\"]\n",
    "        elif ckpt.endswith(\"safetensors\"):\n",
    "            sd = load_safetensors(ckpt)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        msg = None\n",
    "\n",
    "        m, u = model.load_state_dict(sd, strict=False)\n",
    "\n",
    "        if len(m) > 0 and verbose:\n",
    "            print(\"missing keys:\")\n",
    "            print(m)\n",
    "        if len(u) > 0 and verbose:\n",
    "            print(\"unexpected keys:\")\n",
    "            print(u)\n",
    "    else:\n",
    "        msg = None\n",
    "\n",
    "    model = initial_model_load(model)\n",
    "    model.eval()\n",
    "    return model, msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sampling(\n",
    "    key=1,\n",
    "    img2img_strength: Optional[float] = None,\n",
    "    specify_num_samples: bool = True,\n",
    "    stage2strength: Optional[float] = None,\n",
    "    options: Optional[Dict[str, int]] = None,\n",
    "):\n",
    "    options = {} if options is None else options\n",
    "\n",
    "    num_rows, num_cols = 1, 1\n",
    "    if specify_num_samples:\n",
    "        num_cols = st.number_input(\n",
    "            f\"num cols #{key}\", value=num_cols, min_value=1, max_value=10\n",
    "        )\n",
    "\n",
    "    steps = st.number_input(\n",
    "        f\"steps #{key}\", value=options.get(\"num_steps\", 50), min_value=1, max_value=1000\n",
    "    )\n",
    "    sampler = [\"EulerEDMSampler\",\n",
    "            \"HeunEDMSampler\",\n",
    "            \"EulerAncestralSampler\",\n",
    "            \"DPMPP2SAncestralSampler\",\n",
    "            \"DPMPP2MSampler\",\n",
    "            \"LinearMultistepSampler\",\n",
    "        ][0]\n",
    "        \n",
    "    discretization = [\"LegacyDDPMDiscretization\",\n",
    "            \"EDMDiscretization\"][0]\n",
    "\n",
    "    discretization_config = get_discretization(discretization, options=options, key=key)\n",
    "\n",
    "    guider_config = get_guider(options=options, key=key)\n",
    "\n",
    "    sampler = get_sampler(sampler, steps, discretization_config, guider_config, key=key)\n",
    "    if img2img_strength is not None:\n",
    "        sampler.discretization = Img2ImgDiscretizationWrapper(\n",
    "            sampler.discretization, strength=img2img_strength\n",
    "        )\n",
    "    if stage2strength is not None:\n",
    "        sampler.discretization = Txt2NoisyDiscretizationWrapper(\n",
    "            sampler.discretization, strength=stage2strength, original_steps=steps\n",
    "        )\n",
    "    return sampler, num_rows, num_cols\n",
    "\n",
    "\n",
    "def get_discretization(discretization, options, key=1):\n",
    "    if discretization == \"LegacyDDPMDiscretization\":\n",
    "        discretization_config = {\n",
    "            \"target\": \"sgm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization\",\n",
    "        }\n",
    "    elif discretization == \"EDMDiscretization\":\n",
    "        sigma_min = st.sidebar.number_input(\n",
    "            f\"sigma_min #{key}\", value=options.get(\"sigma_min\", 0.03)\n",
    "        )  # 0.0292\n",
    "        sigma_max = st.sidebar.number_input(\n",
    "            f\"sigma_max #{key}\", value=options.get(\"sigma_max\", 14.61)\n",
    "        )  # 14.6146\n",
    "        rho = st.sidebar.number_input(f\"rho #{key}\", value=options.get(\"rho\", 3.0))\n",
    "        discretization_config = {\n",
    "            \"target\": \"sgm.modules.diffusionmodules.discretizer.EDMDiscretization\",\n",
    "            \"params\": {\n",
    "                \"sigma_min\": sigma_min,\n",
    "                \"sigma_max\": sigma_max,\n",
    "                \"rho\": rho,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    return discretization_config\n",
    "\n",
    "\n",
    "def get_sampler(sampler_name, steps, discretization_config, guider_config, key=1):\n",
    "    if sampler_name == \"EulerEDMSampler\" or sampler_name == \"HeunEDMSampler\":\n",
    "        s_churn = st.sidebar.number_input(f\"s_churn #{key}\", value=0.0, min_value=0.0)\n",
    "        s_tmin = st.sidebar.number_input(f\"s_tmin #{key}\", value=0.0, min_value=0.0)\n",
    "        s_tmax = st.sidebar.number_input(f\"s_tmax #{key}\", value=999.0, min_value=0.0)\n",
    "        s_noise = st.sidebar.number_input(f\"s_noise #{key}\", value=1.0, min_value=0.0)\n",
    "\n",
    "        if sampler_name == \"EulerEDMSampler\":\n",
    "            sampler = EulerEDMSampler(\n",
    "                num_steps=steps,\n",
    "                discretization_config=discretization_config,\n",
    "                guider_config=guider_config,\n",
    "                s_churn=s_churn,\n",
    "                s_tmin=s_tmin,\n",
    "                s_tmax=s_tmax,\n",
    "                s_noise=s_noise,\n",
    "                verbose=True,\n",
    "            )\n",
    "        elif sampler_name == \"HeunEDMSampler\":\n",
    "            sampler = HeunEDMSampler(\n",
    "                num_steps=steps,\n",
    "                discretization_config=discretization_config,\n",
    "                guider_config=guider_config,\n",
    "                s_churn=s_churn,\n",
    "                s_tmin=s_tmin,\n",
    "                s_tmax=s_tmax,\n",
    "                s_noise=s_noise,\n",
    "                verbose=True,\n",
    "            )\n",
    "    elif (\n",
    "        sampler_name == \"EulerAncestralSampler\"\n",
    "        or sampler_name == \"DPMPP2SAncestralSampler\"\n",
    "    ):\n",
    "        s_noise = st.sidebar.number_input(\"s_noise\", value=1.0, min_value=0.0)\n",
    "        eta = st.sidebar.number_input(\"eta\", value=1.0, min_value=0.0)\n",
    "\n",
    "        if sampler_name == \"EulerAncestralSampler\":\n",
    "            sampler = EulerAncestralSampler(\n",
    "                num_steps=steps,\n",
    "                discretization_config=discretization_config,\n",
    "                guider_config=guider_config,\n",
    "                eta=eta,\n",
    "                s_noise=s_noise,\n",
    "                verbose=True,\n",
    "            )\n",
    "        elif sampler_name == \"DPMPP2SAncestralSampler\":\n",
    "            sampler = DPMPP2SAncestralSampler(\n",
    "                num_steps=steps,\n",
    "                discretization_config=discretization_config,\n",
    "                guider_config=guider_config,\n",
    "                eta=eta,\n",
    "                s_noise=s_noise,\n",
    "                verbose=True,\n",
    "            )\n",
    "    elif sampler_name == \"DPMPP2MSampler\":\n",
    "        sampler = DPMPP2MSampler(\n",
    "            num_steps=steps,\n",
    "            discretization_config=discretization_config,\n",
    "            guider_config=guider_config,\n",
    "            verbose=True,\n",
    "        )\n",
    "    elif sampler_name == \"LinearMultistepSampler\":\n",
    "        order = st.sidebar.number_input(\"order\", value=4, min_value=1)\n",
    "        sampler = LinearMultistepSampler(\n",
    "            num_steps=steps,\n",
    "            discretization_config=discretization_config,\n",
    "            guider_config=guider_config,\n",
    "            order=order,\n",
    "            verbose=True,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"unknown sampler {sampler_name}!\")\n",
    "\n",
    "    return sampler\n",
    "\n",
    "\n",
    "def get_guider(options, key):\n",
    "    guider = [\n",
    "            \"VanillaCFG\",\n",
    "            \"IdentityGuider\",\n",
    "            \"LinearPredictionGuider\",\n",
    "            \"TrianglePredictionGuider\",\n",
    "        ][0]\n",
    "\n",
    "    additional_guider_kwargs = options.pop(\"additional_guider_kwargs\", {})\n",
    "\n",
    "    if guider == \"IdentityGuider\":\n",
    "        guider_config = {\n",
    "            \"target\": \"sgm.modules.diffusionmodules.guiders.IdentityGuider\"\n",
    "        }\n",
    "    elif guider == \"VanillaCFG\":\n",
    "        scale = st.number_input(\n",
    "            f\"cfg-scale #{key}\",\n",
    "            value=options.get(\"cfg\", 5.0),\n",
    "            min_value=0.0,\n",
    "        )\n",
    "\n",
    "        guider_config = {\n",
    "            \"target\": \"sgm.modules.diffusionmodules.guiders.VanillaCFG\",\n",
    "            \"params\": {\n",
    "                \"scale\": scale,\n",
    "                **additional_guider_kwargs,\n",
    "            },\n",
    "        }\n",
    "    elif guider == \"LinearPredictionGuider\":\n",
    "        max_scale = st.number_input(\n",
    "            f\"max-cfg-scale #{key}\",\n",
    "            value=options.get(\"cfg\", 1.5),\n",
    "            min_value=1.0,\n",
    "        )\n",
    "        min_scale = st.sidebar.number_input(\n",
    "            f\"min guidance scale\",\n",
    "            value=options.get(\"min_cfg\", 1.0),\n",
    "            min_value=1.0,\n",
    "            max_value=10.0,\n",
    "        )\n",
    "\n",
    "        guider_config = {\n",
    "            \"target\": \"sgm.modules.diffusionmodules.guiders.LinearPredictionGuider\",\n",
    "            \"params\": {\n",
    "                \"max_scale\": max_scale,\n",
    "                \"min_scale\": min_scale,\n",
    "                \"num_frames\": options[\"num_frames\"],\n",
    "                **additional_guider_kwargs,\n",
    "            },\n",
    "        }\n",
    "    elif guider == \"TrianglePredictionGuider\":\n",
    "        max_scale = st.number_input(\n",
    "            f\"max-cfg-scale #{key}\",\n",
    "            value=options.get(\"cfg\", 2.5),\n",
    "            min_value=1.0,\n",
    "            max_value=10.0,\n",
    "        )\n",
    "        min_scale = st.sidebar.number_input(\n",
    "            f\"min guidance scale\",\n",
    "            value=options.get(\"min_cfg\", 1.0),\n",
    "            min_value=1.0,\n",
    "            max_value=10.0,\n",
    "        )\n",
    "\n",
    "        guider_config = {\n",
    "            \"target\": \"sgm.modules.diffusionmodules.guiders.TrianglePredictionGuider\",\n",
    "            \"params\": {\n",
    "                \"max_scale\": max_scale,\n",
    "                \"min_scale\": min_scale,\n",
    "                \"num_frames\": options[\"num_frames\"],\n",
    "                **additional_guider_kwargs,\n",
    "            },\n",
    "        }\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return guider_config\n",
    "\n",
    "\n",
    "def get_batch(\n",
    "    keys,\n",
    "    value_dict: dict,\n",
    "    N: Union[List, ListConfig],\n",
    "    device: str = \"cuda\",\n",
    "    T: int = None,\n",
    "    additional_batch_uc_fields: List[str] = [],\n",
    "):\n",
    "    # Hardcoded demo setups; might undergo some changes in the future\n",
    "\n",
    "    batch = {}\n",
    "    batch_uc = {}\n",
    "\n",
    "    for key in keys:\n",
    "        if key == \"txt\":\n",
    "            batch[\"txt\"] = [value_dict[\"prompt\"]] * math.prod(N)\n",
    "\n",
    "            batch_uc[\"txt\"] = [value_dict[\"negative_prompt\"]] * math.prod(N)\n",
    "\n",
    "        elif key == \"original_size_as_tuple\":\n",
    "            batch[\"original_size_as_tuple\"] = (\n",
    "                torch.tensor([value_dict[\"orig_height\"], value_dict[\"orig_width\"]])\n",
    "                .to(device)\n",
    "                .repeat(math.prod(N), 1)\n",
    "            )\n",
    "        elif key == \"crop_coords_top_left\":\n",
    "            batch[\"crop_coords_top_left\"] = (\n",
    "                torch.tensor(\n",
    "                    [value_dict[\"crop_coords_top\"], value_dict[\"crop_coords_left\"]]\n",
    "                )\n",
    "                .to(device)\n",
    "                .repeat(math.prod(N), 1)\n",
    "            )\n",
    "        elif key == \"aesthetic_score\":\n",
    "            batch[\"aesthetic_score\"] = (\n",
    "                torch.tensor([value_dict[\"aesthetic_score\"]])\n",
    "                .to(device)\n",
    "                .repeat(math.prod(N), 1)\n",
    "            )\n",
    "            batch_uc[\"aesthetic_score\"] = (\n",
    "                torch.tensor([value_dict[\"negative_aesthetic_score\"]])\n",
    "                .to(device)\n",
    "                .repeat(math.prod(N), 1)\n",
    "            )\n",
    "\n",
    "        elif key == \"target_size_as_tuple\":\n",
    "            batch[\"target_size_as_tuple\"] = (\n",
    "                torch.tensor([value_dict[\"target_height\"], value_dict[\"target_width\"]])\n",
    "                .to(device)\n",
    "                .repeat(math.prod(N), 1)\n",
    "            )\n",
    "        elif key == \"fps\":\n",
    "            batch[key] = (\n",
    "                torch.tensor([value_dict[\"fps\"]]).to(device).repeat(math.prod(N))\n",
    "            )\n",
    "        elif key == \"fps_id\":\n",
    "            batch[key] = (\n",
    "                torch.tensor([value_dict[\"fps_id\"]]).to(device).repeat(math.prod(N))\n",
    "            )\n",
    "        elif key == \"motion_bucket_id\":\n",
    "            batch[key] = (\n",
    "                torch.tensor([value_dict[\"motion_bucket_id\"]])\n",
    "                .to(device)\n",
    "                .repeat(math.prod(N))\n",
    "            )\n",
    "        elif key == \"pool_image\":\n",
    "            batch[key] = repeat(value_dict[key], \"1 ... -> b ...\", b=math.prod(N)).to(\n",
    "                device, dtype=torch.half\n",
    "            )\n",
    "        elif key == \"cond_aug\":\n",
    "            batch[key] = repeat(\n",
    "                torch.tensor([value_dict[\"cond_aug\"]]).to(\"cuda\"),\n",
    "                \"1 -> b\",\n",
    "                b=math.prod(N),\n",
    "            )\n",
    "        elif key == \"cond_frames\":\n",
    "            batch[key] = repeat(value_dict[\"cond_frames\"], \"1 ... -> b ...\", b=N[0])\n",
    "        elif key == \"cond_frames_without_noise\":\n",
    "            batch[key] = repeat(\n",
    "                value_dict[\"cond_frames_without_noise\"], \"1 ... -> b ...\", b=N[0]\n",
    "            )\n",
    "        elif key == \"polars_rad\":\n",
    "            batch[key] = torch.tensor(value_dict[\"polars_rad\"]).to(device).repeat(N[0])\n",
    "        elif key == \"azimuths_rad\":\n",
    "            batch[key] = (\n",
    "                torch.tensor(value_dict[\"azimuths_rad\"]).to(device).repeat(N[0])\n",
    "            )\n",
    "        else:\n",
    "            batch[key] = value_dict[key]\n",
    "\n",
    "    if T is not None:\n",
    "        batch[\"num_video_frames\"] = T\n",
    "\n",
    "    for key in batch.keys():\n",
    "        if key not in batch_uc and isinstance(batch[key], torch.Tensor):\n",
    "            batch_uc[key] = torch.clone(batch[key])\n",
    "        elif key in additional_batch_uc_fields and key not in batch_uc:\n",
    "            batch_uc[key] = copy.copy(batch[key])\n",
    "    return batch, batch_uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def do_img2img(\n",
    "    img,\n",
    "    model,\n",
    "    sampler,\n",
    "    value_dict,\n",
    "    num_samples,\n",
    "    force_uc_zero_embeddings: Optional[List] = None,\n",
    "    force_cond_zero_embeddings: Optional[List] = None,\n",
    "    additional_kwargs={},\n",
    "    offset_noise_level: int = 0.0,\n",
    "    return_latents=False,\n",
    "    skip_encode=False,\n",
    "    filter=None,\n",
    "    add_noise=True,\n",
    "):\n",
    "    precision_scope = autocast\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                load_model(model.conditioner)\n",
    "                batch, batch_uc = get_batch(\n",
    "                    get_unique_embedder_keys_from_conditioner(model.conditioner),\n",
    "                    value_dict,\n",
    "                    [num_samples],\n",
    "                )\n",
    "                c, uc = model.conditioner.get_unconditional_conditioning(\n",
    "                    batch,\n",
    "                    batch_uc=batch_uc,\n",
    "                    force_uc_zero_embeddings=force_uc_zero_embeddings,\n",
    "                    force_cond_zero_embeddings=force_cond_zero_embeddings,\n",
    "                )           # keys: crossattn [B, 77, 2048], vector: [B, 2816]\n",
    "                unload_model(model.conditioner)\n",
    "                for k in c:\n",
    "                    c[k], uc[k] = map(lambda y: y[k][:num_samples].to(\"cuda\"), (c, uc))\n",
    "\n",
    "                for k in additional_kwargs:\n",
    "                    c[k] = uc[k] = additional_kwargs[k]\n",
    "                if skip_encode:\n",
    "                    z = img\n",
    "                else:\n",
    "                    load_model(model.first_stage_model)\n",
    "                    z = model.encode_first_stage(img)\n",
    "                    unload_model(model.first_stage_model)\n",
    "\n",
    "                \n",
    "                unload_model(img)\n",
    "\n",
    "                noise = torch.randn_like(z)\n",
    "\n",
    "                sigmas = sampler.discretization(sampler.num_steps).cuda()\n",
    "                sigma = sigmas[0]\n",
    "\n",
    "     \n",
    "                if offset_noise_level > 0.0:\n",
    "                    noise = noise + offset_noise_level * append_dims(\n",
    "                        torch.randn(z.shape[0], device=z.device), z.ndim\n",
    "                    )\n",
    "                if add_noise:\n",
    "                    noised_z = z + noise * append_dims(sigma, z.ndim).cuda()\n",
    "                    noised_z = noised_z / torch.sqrt(\n",
    "                        1.0 + sigmas[0] ** 2.0\n",
    "                    )  # Note: hardcoded to DDPM-like scaling. need to generalize later.\n",
    "                else:\n",
    "                    noised_z = z / torch.sqrt(1.0 + sigmas[0] ** 2.0)\n",
    "\n",
    "                def denoiser(x, sigma, c):\n",
    "                    return model.denoiser(model.model, x, sigma, c)\n",
    "\n",
    "                load_model(model.denoiser)\n",
    "                load_model(model.model)\n",
    "                samples_z = sampler(denoiser, noised_z, cond=c, uc=uc)\n",
    "                unload_model(model.model)\n",
    "                unload_model(model.denoiser)\n",
    "\n",
    "                load_model(model.first_stage_model)\n",
    "                samples_x = model.decode_first_stage(samples_z)\n",
    "                unload_model(model.first_stage_model)\n",
    "                samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                if filter is not None:\n",
    "                    samples = filter(samples)\n",
    "\n",
    "                grid = torch.stack([samples]) \n",
    "                grid = rearrange(grid, \"n b c h w -> (n h) (b w) c\")\n",
    "                if return_latents:\n",
    "                    return samples, samples_z\n",
    "                return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def run_img2img(\n",
    "    model,\n",
    "    prompt_dict,\n",
    "    is_legacy=False,\n",
    "    return_latents=False,\n",
    "    filter=None,\n",
    "):\n",
    "    img = load_img(prompt_dict[\"img_path\"], prompt_dict[\"resolution\"])\n",
    "\n",
    "    H, W = img.shape[2], img.shape[3]\n",
    "\n",
    "    init_dict = {\n",
    "        \"orig_width\": W,\n",
    "        \"orig_height\": H,\n",
    "        \"target_width\": W,\n",
    "        \"target_height\": H,\n",
    "    }\n",
    "\n",
    "    print(init_dict)\n",
    "\n",
    "    value_dict = init_embedder_options(\n",
    "        get_unique_embedder_keys_from_conditioner(model.conditioner),\n",
    "        init_dict,\n",
    "        prompt=prompt_dict[\"prompt\"],\n",
    "        negative_prompt=prompt_dict[\"negative_prompt\"],\n",
    "    )\n",
    "\n",
    "    sampler, num_rows, num_cols = init_sampling(\n",
    "        img2img_strength=prompt_dict[\"strength\"],\n",
    "        stage2strength=prompt_dict[\"stage2strength\"],\n",
    "    )\n",
    "    num_samples = num_rows * num_cols\n",
    "\n",
    "    out = do_img2img(\n",
    "        repeat(img, \"1 ... -> n ...\", n=num_samples),\n",
    "        model,\n",
    "        sampler,\n",
    "        value_dict,\n",
    "        num_samples,\n",
    "        force_uc_zero_embeddings=[\"txt\"] if not is_legacy else [],\n",
    "        return_latents=return_latents,\n",
    "        filter=filter,\n",
    "    )\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowvram_mode = True\n",
    "\n",
    "def init(version_dict, load_ckpt=True, load_filter=True):\n",
    "    config = version_dict[\"config\"]\n",
    "    ckpt = version_dict[\"ckpt\"]\n",
    "\n",
    "    config = OmegaConf.load(config)\n",
    "    model, msg = load_model_from_config(config, ckpt if load_ckpt else None)\n",
    "\n",
    "    print(msg)\n",
    "    filters = None\n",
    "    if load_filter:\n",
    "        filters = DeepFloydDataFiltering(verbose=False)\n",
    "\n",
    "    return model, filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "# MODEL_VERSION = \"SDXL-refiner-1.0\"  # Set model version\n",
    "MODEL_VERSION = \"SDXL-base-1.0\"  # Set model version\n",
    "SEED = 42  # Set seed for reproducibility\n",
    "SAVE_PATH = \"outputs/img2img/\"\n",
    "\n",
    "# Initialize\n",
    "seed_everything(SEED)\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Model and Sampler Initialization (assumes helper functions are imported)\n",
    "version_dict = VERSION2SPECS[MODEL_VERSION]\n",
    "is_legacy = version_dict[\"is_legacy\"]\n",
    "model, filters = init(version_dict)\n",
    "load_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"../datasets/imgs\"\n",
    "\n",
    "prompt_dict = {\n",
    "    \"img_path\": os.path.join(img_dir, \"Paris - 5.jpg\"),\n",
    "    \"resolution\": (1792, 1152), # H, W\n",
    "    # \"resolution\": (1080, 2000), # H, W\n",
    "    # \"resolution\": (1600, 1200), # H, W\n",
    "    # \"resolution\": (1200, 1600), # H, W\n",
    "    # \"resolution\": (1152, 1792), # W, H\n",
    "    \"prompt\": \"Painting in Monet's style, muted colors, Impressionistic strokes, detailed, 8k\",\n",
    "    # \"prompt\": \"Painting of ... in Monet's style, muted colors, Impressionistic art strokes, detailed, 8k\",\n",
    "    \"negative_prompt\": \"\",\n",
    "    \"strength\": 0.75,      # img2img strength: recommended starting point 0.75 for base, 0.45 for refiner\n",
    "    \"stage2strength\": None,\n",
    "    # \"stage2strength\": 0.15,\n",
    "}\n",
    "\n",
    "unload_model(model)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "samples = run_img2img(model, prompt_dict, is_legacy, False, filters).cpu()\n",
    "\n",
    "# Save Output\n",
    "for i, sample in enumerate(samples):\n",
    "    sample = (255.0 * rearrange(sample.cpu().numpy(), \"c h w -> h w c\")).astype(np.uint8)\n",
    "    \n",
    "    # Construct initial output path\n",
    "    base_filename = os.path.basename(prompt_dict[\"img_path\"]).split(\".\")[0]\n",
    "    output_path = os.path.join(SAVE_PATH, f\"{base_filename}_{prompt_dict['strength']:.2f}_{i:03}.png\")\n",
    "    \n",
    "    # Check if file exists, and increment 'i' if necessary\n",
    "    while os.path.exists(output_path):\n",
    "        i += 1\n",
    "        output_path = os.path.join(SAVE_PATH, f\"{base_filename}_{prompt_dict['strength']:.2f}_{i:03}.png\")\n",
    "    \n",
    "    # Save the image to the unique path\n",
    "    Image.fromarray(sample).save(output_path)\n",
    "    print(f\"Saved image to {output_path}\")\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    # Load the image\n",
    "    img = mpimg.imread(output_path)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.Image(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_model(model)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tensors consuming GPU memory\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
    "            if obj.is_cuda:\n",
    "                print(f\"Tensor: {type(obj)}, Shape: {obj.size()}, Device: {obj.device}\")\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
