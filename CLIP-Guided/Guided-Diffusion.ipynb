{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Guided Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import gc\n",
    "import io\n",
    "import math\n",
    "import sys\n",
    "from IPython import display\n",
    "import lpips\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import clip\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the GPU status\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "### Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'checkpoints/'\n",
    "\n",
    "diffusion_model = '256x256_diffusion_uncond' #'512x512_diffusion_uncond_finetune_008100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep_respacing = 'ddim50' # Modify this value to decrease the number of timesteps.\n",
    "timestep_respacing = '50'\n",
    "diffusion_steps = 1000\n",
    "\n",
    "model_config = model_and_diffusion_defaults()\n",
    "if diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': diffusion_steps,\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': timestep_respacing,\n",
    "        'image_size': 512,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 256,\n",
    "        'num_head_channels': 64,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_fp16': True,\n",
    "        'use_scale_shift_norm': True,\n",
    "        'use_checkpoint': True,\n",
    "    })\n",
    "elif diffusion_model == '256x256_diffusion_uncond':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': diffusion_steps,\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': timestep_respacing,\n",
    "        'image_size': 256,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 256,\n",
    "        'num_head_channels': 64,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_fp16': True,\n",
    "        'use_scale_shift_norm': True,\n",
    "        'use_checkpoint': True,\n",
    "    })\n",
    "side_x = side_y = model_config['image_size']\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "model.load_state_dict(torch.load(f'{model_path}{diffusion_model}.pt', map_location='cpu'))\n",
    "model.requires_grad_(False).eval().to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
    "        param.requires_grad_()\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "clip_size = clip_model.visual.input_resolution\n",
    "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_guidance_scale = 15000 # 5000 (new:15000) - Controls how much the image should look like the prompt.\n",
    "tv_scale = 2500 # 500 (new:2500) - Controls the smoothness of the final output.\n",
    "range_scale = 100 # 100 - Controls how far out of range RGB values are allowed to be.\n",
    "sat_scale = 0 # 0 - Controls how much saturation is allowed. From nshepperd's JAX notebook, though not sure if it's doing anything right now...\n",
    "cutn = 16 # 16 - Controls how many crops to take from the image. Increase for higher quality.\n",
    "cutn_batches = 2 # 2 - Accumulate CLIP gradient from multiple batches of cuts [Can help with OOM errors / Low VRAM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_augs = False # False - Controls whether to skip torchvision augmentations\n",
    "randomize_class = True # True - Controls whether the imagenet class is randomly changed each iteration\n",
    "clip_denoised = False # False - Determines whether CLIP discriminates a noisy or denoised image\n",
    "clamp_max = 0.035 # 0.05 (new:0.035)\n",
    "\n",
    "fuzzy_prompt = False # False - Controls whether to add multiple noisy prompts to the prompt losses\n",
    "rand_mag = 0.05 # 0.05 - Controls the magnitude of the random noise\n",
    "eta = 0.5 # 0.5 - DDIM hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
    "        vals = prompt.rsplit(':', 2)\n",
    "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
    "    else:\n",
    "        vals = prompt.rsplit(':', 1)\n",
    "    vals = vals + ['', '1'][len(vals):]\n",
    "    return vals[0], float(vals[1])\n",
    "\n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    "\n",
    "    input = input.reshape([n * c, 1, h, w])\n",
    "\n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    "\n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    "\n",
    "    input = input.reshape([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, skip_augs=False):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.skip_augs = skip_augs\n",
    "        self.augs = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomAffine(degrees=15, translate=(0.1, 0.1), interpolation=TF.InterpolationMode.BILINEAR),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomGrayscale(p=0.35),\n",
    "        ])\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = T.Pad(input.shape[2]//4, fill=0)(input)\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "\n",
    "        cutouts = []\n",
    "        for ch in range(cutn):\n",
    "            if ch > cutn - cutn//4:\n",
    "                cutout = input.clone()\n",
    "            else:\n",
    "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
    "                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n",
    "                offsety = torch.randint(0, abs(sideY - size + 1), ())\n",
    "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "\n",
    "            if not self.skip_augs:\n",
    "                cutout = self.augs(cutout)\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "            del cutout\n",
    "\n",
    "        cutouts = torch.cat(cutouts, dim=0)\n",
    "        return cutouts\n",
    "\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def range_loss(input):\n",
    "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
    "\n",
    "def unitwise_norm(x, norm_type=2.0):\n",
    "    if x.ndim <= 1:\n",
    "        return x.norm(norm_type)\n",
    "    else:\n",
    "        # works for nn.ConvNd and nn,Linear where output dim is first in the kernel/weight tensor\n",
    "        # might need special cases for other weights (possibly MHA) where this may not be true\n",
    "        return x.norm(norm_type, dim=tuple(range(1, x.ndim)), keepdim=True)\n",
    "\n",
    "def adaptive_clip_grad(parameters, clip_factor=0.01, eps=1e-3, norm_type=2.0):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    for p in parameters:\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        p_data = p.detach()\n",
    "        g_data = p.grad.detach()\n",
    "        max_norm = unitwise_norm(p_data, norm_type=norm_type).clamp_(min=eps).mul_(clip_factor)\n",
    "        grad_norm = unitwise_norm(g_data, norm_type=norm_type)\n",
    "        clipped_grad = g_data * (max_norm / grad_norm.clamp(min=1e-6))\n",
    "        new_grads = torch.where(grad_norm < max_norm, g_data, clipped_grad)\n",
    "        p.grad.detach().copy_(new_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_run(init_image, init_scale, skip_timesteps, text_prompts, image_prompts, seed=None, batch_size=1, n_batches=1, display_rate=1, save_dir=\"results\"):\n",
    "    loss_values = []\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    make_cutouts = MakeCutouts(clip_size, cutn, skip_augs=skip_augs)\n",
    "    target_embeds, weights = [], []\n",
    "\n",
    "    for prompt in text_prompts:\n",
    "        txt, weight = parse_prompt(prompt)\n",
    "        txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
    "        target_embeds.append(txt)\n",
    "        weights.append(weight)\n",
    "\n",
    "    for prompt in image_prompts:\n",
    "        path, weight = parse_prompt(prompt)\n",
    "        img = Image.open(fetch(path)).convert('RGB')\n",
    "        img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n",
    "        batch = make_cutouts(TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n",
    "        embed = clip_model.encode_image(normalize(batch)).float()\n",
    "        target_embeds.append(embed)\n",
    "        weights.extend([weight / cutn] * cutn)\n",
    "\n",
    "    target_embeds = torch.cat(target_embeds)\n",
    "    weights = torch.tensor(weights, device=device)\n",
    "    if weights.sum().abs() < 1e-3:\n",
    "        raise RuntimeError('The weights must not sum to 0.')\n",
    "    weights /= weights.sum().abs()\n",
    "\n",
    "    init = None\n",
    "    if init_image is not None:\n",
    "        init = Image.open(fetch(init_image)).convert('RGB')\n",
    "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
    "        init = TF.to_tensor(init).unsqueeze(0).expand(batch_size, -1, -1, -1).to(device).mul(2).sub(1)\n",
    "\n",
    "    cur_t = None\n",
    "    def cond_fn(x, t, out, y=None):\n",
    "        n = x.shape[0]\n",
    "        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "        x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "        x_in_grad = torch.zeros_like(x_in)\n",
    "\n",
    "        for i in range(cutn_batches):\n",
    "            clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
    "            image_embeds = clip_model.encode_image(clip_in).float()\n",
    "            dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
    "            dists = dists.view([cutn, n, -1])\n",
    "            losses = dists.mul(weights).sum(2).mean(0)\n",
    "            loss_values.append(losses.sum().item())\n",
    "            x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches\n",
    "\n",
    "        tv_losses = tv_loss(x_in)\n",
    "        range_losses = range_loss(out['pred_xstart'])\n",
    "        sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()\n",
    "        loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n",
    "        if init is not None and init_scale:\n",
    "            init_losses = lpips_model(x_in, init)\n",
    "            loss = loss + init_losses.sum() * init_scale\n",
    "        x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
    "        grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
    "        adaptive_clip_grad([x])\n",
    "        magnitude = grad.square().mean().sqrt()\n",
    "        return grad * magnitude.clamp(max=clamp_max) / magnitude\n",
    "\n",
    "    if model_config['timestep_respacing'].startswith('ddim'):\n",
    "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "    else:\n",
    "        sample_fn = diffusion.p_sample_loop_progressive\n",
    "\n",
    "    original_target_embeds = target_embeds.clone()\n",
    "    for i in range(n_batches):\n",
    "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
    "\n",
    "        if fuzzy_prompt:\n",
    "            target_embeds = original_target_embeds.clone() +  torch.randn_like(target_embeds).cuda() * rand_mag\n",
    "\n",
    "        # if perlin_init:\n",
    "        #     init = regen_perlin()\n",
    "\n",
    "        if model_config['timestep_respacing'].startswith('ddim'):\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, side_y, side_x),\n",
    "                clip_denoised=clip_denoised,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_timesteps,\n",
    "                init_image=init,\n",
    "                randomize_class=randomize_class,\n",
    "                eta=eta,\n",
    "                cond_fn_with_grad=True,\n",
    "            )\n",
    "        else:\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, side_y, side_x),\n",
    "                clip_denoised=clip_denoised,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_timesteps,\n",
    "                init_image=init,\n",
    "                randomize_class=randomize_class,\n",
    "                cond_fn_with_grad=True,\n",
    "            )\n",
    "\n",
    "        for j, sample in enumerate(samples):\n",
    "            display.clear_output(wait=True)\n",
    "            cur_t -= 1\n",
    "            if j % display_rate == 0 or cur_t == -1:\n",
    "                for k, image in enumerate(sample['pred_xstart']):\n",
    "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
    "                    current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n",
    "                    filename = f'progress_batch{i:05}_iteration{j:05}_output{k:05}_{current_time}.png'\n",
    "                    image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n",
    "                    display.display(image)\n",
    "                    # img_path = os.path.join(save_dir, 'verbose', filename)\n",
    "                    # image.save(img_path)\n",
    "                    # display.display(display.Image(img_path))\n",
    "\n",
    "            # save final\n",
    "        img_path = os.path.join(save_dir, filename)\n",
    "        image.save(img_path)\n",
    "\n",
    "        plt.plot(np.array(loss_values), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def full_img_path(image_list, image_dir=\"prompts/imgs\"):\n",
    "    return [os.path.join(image_dir, img_name) for img_name in image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompts = [\n",
    "    # \"an abstract painting of 'ravioli on a plate'\",\n",
    "    # 'cyberpunk wizard',\n",
    "    # 'the gateway to eternal dread (painting)',\n",
    "    # \"a painting of Aalto university building in Monet's style\"\n",
    "    \"a painting of mountain in Monet's style\",\n",
    "    # \"A painting in Monet's style\",\n",
    "]\n",
    "\n",
    "image_prompts = [\n",
    "    'Mountain.jpg'\n",
    "]\n",
    "\n",
    "image_prompts = full_img_path(image_prompts)\n",
    "\n",
    "init_image = image_prompts[0] # None - URL or local path\n",
    "init_scale = 1000 # 0 - This enhances the effect of the init image, a good value is 1000\n",
    "skip_timesteps = 8 # 10 (new:5) - Controls the starting point along the diffusion timesteps\n",
    "\n",
    "\n",
    "display_rate = 1\n",
    "n_batches = 1 # 1 - Controls how many consecutive batches of images are generated\n",
    "batch_size = 1 # 1 - Controls how many images are generated in parallel in a batch\n",
    "\n",
    "# seed = 0\n",
    "seed = random.randint(0, 2**32) # Choose a random seed and print it at end of run for reproduction\n",
    "\n",
    "print('seed', seed)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "do_run(init_image, init_scale, skip_timesteps, text_prompts, image_prompts, seed, batch_size, n_batches, display_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
