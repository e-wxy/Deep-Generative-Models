{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7AZNJDdAkxA"
   },
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "Reference:\n",
    "1. L. A. Gatys, A. S. Ecker and M. Bethge, \"Image Style Transfer Using Convolutional Neural Networks,\" 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 2414-2423, doi: 10.1109/CVPR.2016.265. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_image(image_path, size=512):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# De-normalize and convert tensor to image\n",
    "def tensor_to_image(tensor):\n",
    "    unnormalize = transforms.Normalize(\n",
    "        mean=[-2.12, -2.04, -1.8],\n",
    "        std=[4.37, 4.46, 4.44]\n",
    "    )\n",
    "    tensor = unnormalize(tensor.squeeze(0)).clamp(0, 1)\n",
    "    return transforms.ToPILImage()(tensor)\n",
    "\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone().detach()\n",
    "    image = image.numpy().squeeze(0)\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "style_img = load_image(\"datasets/style/Water Lily.jpeg\").to(device)\n",
    "content_img = load_image(\"datasets/photo/Mountain_1.jpg\").to(device)\n",
    "input_img = content_img.clone().to(device)\n",
    "print(\"Content Image Shape:\", content_img.shape)\n",
    "print(\"Style Image Shape:\", style_img.shape)\n",
    "\n",
    "\n",
    "imshow(style_img)\n",
    "imshow(content_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained VGG model\n",
    "cnn = models.vgg19(pretrained=True).to(device)\n",
    "\n",
    "cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, x):\n",
    "        loss = nn.functional.mse_loss(x, self.target)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target, weights=None, loss=nn.MSELoss()):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = []\n",
    "        if weights is None:\n",
    "            self.weights = torch.ones(len(target))\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "        self.loss = loss\n",
    "\n",
    "        for layer_name, feature in target.items():\n",
    "            self.target.append(self.gram_matrix(feature).detach())\n",
    "\n",
    "    def forward(self, x):\n",
    "        loss = 0\n",
    "        for feature, weight, target in zip(x, self.weights, self.target):\n",
    "            G = self.gram_matrix(feature)\n",
    "            loss += weight * self.loss(G, target)\n",
    "        return loss\n",
    "\n",
    "    def gram_matrix(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        features = x.view(b, c, h * w)\n",
    "        G = torch.bmm(features, features.transpose(1, 2))\n",
    "        return G.div(c * h * w)\n",
    "    \n",
    "\n",
    "def gram_matrix(x):\n",
    "    b, c, h, w = x.size()\n",
    "    features = x.view(b, c, h * w)\n",
    "    G = torch.bmm(features, features.transpose(1, 2))\n",
    "    return G.div(c * h * w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG model and extract required layers\n",
    "def get_features(cnn, image, layers:dict=None):\n",
    "    \"\"\" Get the feature representations\n",
    "\n",
    "    Args:\n",
    "        cnn (nn.Moudle): a Convolution Neural Network\n",
    "        image (torch.tensor): input image\n",
    "        layers (dict): dict of representation layres, layer_id -> layer_name\n",
    "\n",
    "    \"\"\"\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',\n",
    "                  '5': 'conv2_1',\n",
    "                  '10': 'conv3_1',\n",
    "                  '19': 'conv4_1',\n",
    "                  '21': 'conv4_2',\n",
    "                  '28': 'conv5_1'}\n",
    "        \n",
    "    features = {}\n",
    "\n",
    "    x = image\n",
    "\n",
    "    # name: int - 0, 1, ...\n",
    "    # layer: - e.g.: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    for id, layer in cnn.features._modules.items():\n",
    "      x = layer(x)\n",
    "      if id in layers:\n",
    "          features[layers[id]] = x\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = {'0': 'conv1_1',\n",
    "                '5': 'conv2_1',\n",
    "                '10': 'conv3_1',\n",
    "                '19': 'conv4_1',\n",
    "                '28': 'conv5_1'}\n",
    "\n",
    "content_layers = {'21': 'conv4_2',}\n",
    "\n",
    "\n",
    "style_feature = get_features(cnn, style_img, style_layers)\n",
    "content_feature = get_features(cnn, content_img, content_layers)\n",
    "\n",
    "s_criterion = StyleLoss(style_feature)\n",
    "c_criterion = ContentLoss(content_feature['conv4_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "zXamlIwIAkxB"
   },
   "outputs": [],
   "source": [
    "def run(cnn, c_criterion, s_criterion, input_img, num_steps=500, loss_weights=[1, 1e4], verbose=10):\n",
    "    optimizer = optim.Adam([input_img.requires_grad_()], lr=0.1)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        features = get_features(cnn, input_img, style_layers | content_layers)\n",
    "\n",
    "        c_loss = c_criterion([features[layer] for _, layer in content_layers.items()][0])\n",
    "        s_loss = s_criterion([features[layer] for _, layer in style_layers.items()])\n",
    "        ### check the orders of layers\n",
    "\n",
    "        loss = loss_weights[0] * c_loss + loss_weights[1] * s_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # input_img.data.clamp_(0, 1)\n",
    "\n",
    "        if step % verbose == 0:\n",
    "            print(f\"Epoch {step}, style: {s_loss * loss_weights[1]}, content: {c_loss * loss_weights[0]}, loss: {loss:.4f}\")\n",
    "            imshow(input_img)\n",
    "\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWfvgrK5AkxC"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXp_r3TtAkxC"
   },
   "outputs": [],
   "source": [
    "# Run style transfer\n",
    "# input_img = torch.randn_like(content_img).to(device)\n",
    "input_img = content_img.clone().to(device)\n",
    "\n",
    "output = run(cnn, c_criterion, s_criterion, input_img, 10000)\n",
    "\n",
    "# Save and show result\n",
    "imshow(output)\n",
    "output_image = tensor_to_image(output.cpu())\n",
    "output_image.save(\"output.jpg\")\n",
    "output_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VonMdSSAkxC"
   },
   "source": [
    "## Reconstruction of Style and Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_content(cnn, content_img, layer_id, num_steps=1000):\n",
    "    model = nn.Sequential()\n",
    "    for i, layer in cnn.features._modules.items():\n",
    "        model.add_module(i, layer)\n",
    "        if id == layer_id:\n",
    "            break\n",
    "    target = model(content_img).detach()\n",
    "\n",
    "    # Initialize random image for reconstruction\n",
    "    # input_img = torch.randn_like(content_img).requires_grad_(True)\n",
    "    input_img = torch.rand_like(content_img)\n",
    "    input_img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(input_img).requires_grad_(True)\n",
    "    optimizer = optim.Adam([input_img], lr=0.2)\n",
    "\n",
    "    # Optimize\n",
    "    print(\"Reconstructing content...\")\n",
    "    for step in range(num_steps):\n",
    "        input_img.data.clamp_(-3, 3)\n",
    "        optimizer.zero_grad()\n",
    "        content_features = model(input_img)\n",
    "        loss = nn.functional.mse_loss(content_features, target)\n",
    "        loss.backward()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "            imshow(input_img)\n",
    "        optimizer.step()\n",
    "    \n",
    "    return input_img\n",
    "            \n",
    "\n",
    "# Content Reconstruction\n",
    "content_reconstruction = reconstruct_content(cnn, content_img, '21')\n",
    "tensor_to_image(content_reconstruction.cpu()).save(\"content_reconstruction.jpg\")\n",
    "\n",
    "imshow(content_reconstruction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_style(cnn, style_img, layer_id, num_steps=1000):\n",
    "    model = nn.Sequential()\n",
    "    for i, layer in cnn.features._modules.items():\n",
    "        model.add_module(i, layer)\n",
    "        if id == layer_id:\n",
    "            break\n",
    "    target = gram_matrix(model(style_img)).detach()\n",
    "\n",
    "    # Initialize random image for reconstruction\n",
    "    # input_img = torch.randn_like(style_img).requires_grad_(True)\n",
    "    input_img = torch.rand_like(style_img)\n",
    "    input_img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(input_img).requires_grad_(True)\n",
    "    optimizer = optim.Adam([input_img], lr=0.2)\n",
    "\n",
    "    # Optimize\n",
    "    print(\"Reconstructing style...\")\n",
    "    for step in range(num_steps):\n",
    "        input_img.data.clamp_(-3, 3)\n",
    "        optimizer.zero_grad()\n",
    "        style_features = gram_matrix(model(input_img))\n",
    "        loss = nn.functional.mse_loss(style_features, target) * 1e6\n",
    "        loss.backward()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "            imshow(input_img)\n",
    "        optimizer.step()\n",
    "    \n",
    "    return input_img\n",
    "            \n",
    "\n",
    "# Style Reconstruction\n",
    "for layer_id in style_layers:\n",
    "    style_reconstruction = reconstruct_style(cnn, style_img, layer_id)\n",
    "    tensor_to_image(style_reconstruction.cpu()).save(f\"style_reconstruction_{layer_id}.jpg\")\n",
    "\n",
    "    imshow(style_reconstruction)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
